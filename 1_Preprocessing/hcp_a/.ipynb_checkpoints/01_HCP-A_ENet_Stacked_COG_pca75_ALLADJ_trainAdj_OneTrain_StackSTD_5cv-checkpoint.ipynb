{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacked Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## IMPORTANT !\n",
    "\n",
    "# In the first order need to set the number of CPU \n",
    "# for calculation before launching (depends on computer's number of cores)\n",
    "n_jobs= 40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import glob\n",
    "import joblib\n",
    "import warnings\n",
    "from datetime import date, datetime\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.model_selection import LeavePGroupsOut\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import resample\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "import scipy.stats as st\n",
    "\n",
    "from nilearn import image as nli\n",
    "from nilearn import plotting\n",
    "\n",
    "from mne.viz import plot_connectivity_circle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def control_features(table_in, control, index): \n",
    "    #table_in should be a table of features, where rows - subjects, columns - features\n",
    "    \n",
    "    if len(table_in.values.shape) == 1: #for pd.Series # for target\n",
    "        \n",
    "        #shrink data to local train index\n",
    "        table_in = table_in.reindex(index = index)\n",
    "        control = control.reindex(index = index)\n",
    "        ind = table_in.index\n",
    "        \n",
    "        #loop\n",
    "        dct_table = {}\n",
    "        dct_lin_models ={}\n",
    "        dct_std_y_models ={}\n",
    "        \n",
    "        col='0'\n",
    "        \n",
    "        y = table_in #target, brain ROI\n",
    "        X = control  #features, like age, sex and/or movements\n",
    "\n",
    "        #Standartize target\n",
    "        std_model_y = StandardScaler()\n",
    "        std_model_y.fit(y.values.reshape(-1, 1))\n",
    "        y = std_model_y.transform(y.values.reshape(-1, 1))\n",
    "        \n",
    "        #reshaping data\n",
    "        if len(X.values.shape) == 1:\n",
    "            X = X.values.reshape(-1, 1)\n",
    "        else:\n",
    "            X = X.values\n",
    "        y = y.reshape(-1, 1).ravel()\n",
    "        \n",
    "        #Standartize X\n",
    "        std_model = StandardScaler()\n",
    "        std_model.fit(X)\n",
    "        X = std_model.transform(X)\n",
    "\n",
    "        #Fit to the training set\n",
    "        model = LinearRegression()\n",
    "        model.fit(X, y)\n",
    "        y_pred = model.predict(X)\n",
    "\n",
    "        y_res = y - y_pred\n",
    "\n",
    "        dct_table[col] = y_res\n",
    "        dct_lin_models[col] = model\n",
    "        dct_std_y_models[col] = std_model_y\n",
    "\n",
    "        df_table = pd.DataFrame(dct_table, index = ind)\n",
    "\n",
    "        \n",
    "    else:\n",
    "            \n",
    "        #shrink data to local train index\n",
    "        table_in = table_in.reindex(index = index)\n",
    "        control = control.reindex(index = index)\n",
    "        ind = table_in.index\n",
    "\n",
    "        #loop\n",
    "        dct_table = {}\n",
    "        dct_lin_models ={}\n",
    "        dct_std_y_models ={}\n",
    "        col_names = table_in.columns\n",
    "\n",
    "        for col in col_names:\n",
    "            y = table_in[col] #target, brain ROI\n",
    "            X = control  #features, like age, sex and/or movements\n",
    "            \n",
    "            #Standartize target\n",
    "            std_model_y = StandardScaler()\n",
    "            std_model_y.fit(y.values.reshape(-1, 1))\n",
    "            y = std_model_y.transform(y.values.reshape(-1, 1)) \n",
    "            \n",
    "            #reshaping data\n",
    "            if len(X.values.shape) == 1:\n",
    "                X = X.values.reshape(-1, 1)\n",
    "            else:\n",
    "                X = X.values\n",
    "            y = y.reshape(-1, 1).ravel()\n",
    "            \n",
    "            #Standartize X\n",
    "            std_model = StandardScaler()\n",
    "            std_model.fit(X)\n",
    "            X = std_model.transform(X)\n",
    "\n",
    "            #Fit to the training set\n",
    "            model = LinearRegression()\n",
    "            model.fit(X, y)\n",
    "            y_pred = model.predict(X)\n",
    "\n",
    "            y_res = y - y_pred\n",
    "\n",
    "            dct_table[col] = y_res\n",
    "            dct_lin_models[col] = model\n",
    "            dct_std_y_models[col] = std_model_y\n",
    "\n",
    "        df_table = pd.DataFrame(dct_table, index = ind)\n",
    "    \n",
    "    return df_table, dct_std_y_models, std_model, dct_lin_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def re_control_features(table_in, control, index, dct_std_y_models, std_model, dct_lin_models):\n",
    "    \n",
    "    if len(table_in.values.shape) == 1: #for pd.Series # for target\n",
    "        \n",
    "        #shrink data to local train index\n",
    "        table_in = table_in.reindex(index = index)\n",
    "        control = control.reindex(index = index)\n",
    "        ind = table_in.index\n",
    "        \n",
    "        #loop\n",
    "        dct_table = {}\n",
    "        \n",
    "        col='0'\n",
    "        \n",
    "        y = table_in #target, brain ROI\n",
    "        X = control  #features, like age, sex and/or movements\n",
    "        \n",
    "        #standartize y\n",
    "        y = dct_std_y_models[col].transform(y.values.reshape(-1, 1))\n",
    "        \n",
    "        #reshaping data\n",
    "        if len(X.values.shape) == 1:\n",
    "            X = X.values.reshape(-1, 1)\n",
    "        else:\n",
    "            X = X.values\n",
    "        y = y.reshape(-1, 1).ravel()\n",
    "\n",
    "        #Standartize X with previous std model\n",
    "        X = std_model.transform(X)\n",
    "\n",
    "        #Fit with previous LinReg model\n",
    "        y_pred =  dct_lin_models[col].predict(X)\n",
    "\n",
    "        y_res = y - y_pred\n",
    "\n",
    "        dct_table[col] = y_res\n",
    "\n",
    "        df_table = pd.DataFrame(dct_table, index = ind)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        #shrink data to local train index\n",
    "        table_in = table_in.reindex(index = index)\n",
    "        control = control.reindex(index = index)\n",
    "        ind = table_in.index\n",
    "\n",
    "        #loop\n",
    "        dct_table = {}\n",
    "        col_names = table_in.columns\n",
    "\n",
    "        for col in col_names:\n",
    "            y = table_in[col] #target, brain ROI\n",
    "            X = control  #features, like age, sex and/or movements\n",
    "\n",
    "            #standartize y\n",
    "            y = dct_std_y_models[col].transform(y.values.reshape(-1, 1))\n",
    "            \n",
    "            #reshaping data\n",
    "            if len(X.values.shape) == 1:\n",
    "                X = X.values.reshape(-1, 1)\n",
    "            else:\n",
    "                X = X.values\n",
    "            y = y.reshape(-1, 1).ravel()\n",
    "\n",
    "            #Standartize X with previous std model\n",
    "            X = std_model.transform(X)\n",
    "\n",
    "            #Fit with previous LinReg model\n",
    "            y_pred =  dct_lin_models[col].predict(X)\n",
    "\n",
    "            y_res = y - y_pred\n",
    "\n",
    "            dct_table[col] = y_res\n",
    "\n",
    "        df_table = pd.DataFrame(dct_table, index = ind)\n",
    "        \n",
    "    return df_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elnet(X, y):\n",
    "\n",
    "    #drop Nan in target and clean this subj from features\n",
    "    y = y.dropna()\n",
    "    X = X.loc[y.index,:]\n",
    "    ind_y = np.array(y.index)\n",
    "      \n",
    "    y_real=y\n",
    "    \n",
    "    #reshaping data\n",
    "    X = X.values\n",
    "    y = y.values.reshape(-1, 1).ravel()\n",
    "    \n",
    "    #fill Nan in X\n",
    "    #X = SimpleImputer(strategy='mean').fit_transform(X)\n",
    "    \n",
    "    #Standartize X\n",
    "    #X = StandardScaler().fit_transform(X)\n",
    "    \n",
    "    # Setup the pipeline steps:\n",
    "    steps = [('elasticnet', ElasticNet(random_state=42))]\n",
    "\n",
    "    # Create the pipeline: pipeline \n",
    "    pipeline = Pipeline(steps)\n",
    "\n",
    "    # Specify the hyperparameter space\n",
    "    parameters = {'elasticnet__alpha': np.logspace(-1, 2, 70),\n",
    "                  'elasticnet__l1_ratio':np.linspace(0,1,25)}\n",
    "\n",
    "    # Create the GridSearchCV object:\n",
    "    gm_cv = GridSearchCV(pipeline, parameters, cv=5, n_jobs=n_jobs)\n",
    "    \n",
    "    # Fit to the training set\n",
    "    gm_cv.fit(X, y)\n",
    "    \n",
    "    #predict new y\n",
    "    y_pred = gm_cv.predict(X)\n",
    "\n",
    "    # Compute and print the metrics\n",
    "    acc = gm_cv.best_score_\n",
    "    bpar = gm_cv.best_params_\n",
    "    model = gm_cv.best_estimator_\n",
    "    mse = mean_squared_error(y_real, y_pred)\n",
    "    mae = mean_absolute_error(y_real, y_pred)\n",
    "    corr, _ = pearsonr(np.array(y_real.values.reshape(-1, 1).ravel(), dtype=float), np.array(y_pred, dtype=float))\n",
    "            \n",
    "    return bpar['elasticnet__alpha'], bpar['elasticnet__l1_ratio'], acc, mse, corr, model, y_pred, mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reaply_ElNet(X, y, model):\n",
    "    # param should be pd.Series with indexes from model\n",
    "    \n",
    "    #drop Nan in target and clean this subj from features\n",
    "    y = y.dropna()\n",
    "    X = X.reindex(index =y.index)\n",
    "    ind_y = np.array(y.index)  # indexes as separate variable \n",
    "    \n",
    "    y_real = y\n",
    "\n",
    "    #reshaping data\n",
    "    X = X.values\n",
    "    y = y.values.reshape(-1, 1).ravel()\n",
    "    \n",
    "    #fill Nan in X\n",
    "    #X = SimpleImputer(strategy='mean').fit_transform(X)\n",
    "    \n",
    "    #Standartize X\n",
    "    #X = StandardScaler().fit_transform(X)\n",
    "    \n",
    "    #predict new y\n",
    "    y_pred = model.predict(X)\n",
    "    \n",
    "    # Compute and print the metrics\n",
    "    bacc = model.score(X, y)\n",
    "    mse = mean_squared_error(y_real, y_pred)\n",
    "    mae = mean_absolute_error(y_real, y_pred) \n",
    "    corr, _ = pearsonr(np.array(y_real.values.reshape(-1, 1).ravel(), dtype=float), np.array(y_pred, dtype=float))\n",
    "    \n",
    "    return y_pred, y_real, ind_y, bacc, mse, corr, mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path to the tables folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='/media/data/HCPAging/data/MLTablesMultCon/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "#demography\n",
    "demo = pd.read_csv(path+'demography.csv', index_col=0)\n",
    "\n",
    "#targets table\n",
    "targ = pd.read_csv(path+'cognition.csv', index_col=0)\n",
    "\n",
    "#features tables as dictionary\n",
    "features = {\n",
    "    'carit1':pd.read_csv(path+'carit-con1.csv', index_col=0),\n",
    "    'carit3':pd.read_csv(path+'carit-con3.csv', index_col=0),\n",
    "    'carit4':pd.read_csv(path+'carit-con4.csv', index_col=0),\n",
    "    \n",
    "    'face1':pd.read_csv(path+'FACENAME_group_table_3EV_con1.csv', index_col=0),\n",
    "    'face2':pd.read_csv(path+'FACENAME_group_table_3EV_con2.csv', index_col=0),\n",
    "    'face3':pd.read_csv(path+'FACENAME_group_table_3EV_con3.csv', index_col=0),\n",
    "    'face4':pd.read_csv(path+'FACENAME_group_table_3EV_con4.csv', index_col=0),\n",
    "    'face5':pd.read_csv(path+'FACENAME_group_table_3EV_con5.csv', index_col=0),\n",
    "    'face6':pd.read_csv(path+'FACENAME_group_table_3EV_con6.csv', index_col=0),\n",
    "    \n",
    "    'vism':pd.read_csv(path+'vism.csv', index_col=0),\n",
    "    \n",
    "    'carit_FC':pd.read_csv(path+'CARIT_taskFC.csv', index_col=0),\n",
    "    'face_FC':pd.read_csv(path+'FACENAME_task_FC_3EV.csv', index_col=0),\n",
    "    'vism_FC':pd.read_csv(path+'VISMOTOR_taskFC.csv', index_col=0),\n",
    "\n",
    "    'cort':pd.read_csv(path+'cort.csv', index_col=0),\n",
    "    'surf':pd.read_csv(path+'surf.csv', index_col=0),\n",
    "    'subc':pd.read_csv(path+'subc.csv', index_col=0),\n",
    "    'VolBrain':pd.read_csv(path+'VolBrain.csv', index_col=0),\n",
    "    \n",
    "    'rest':pd.read_csv(path+'rest_hpass.csv', index_col=0) \n",
    "\n",
    "}\n",
    "\n",
    "#table with movements (mean relative displacement Movement_RelativeRMS_mean.txt)\n",
    "movements = pd.read_csv(path+'movements.csv', index_col=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create tables withcontroling parameters\n",
    "sex_coded = pd.Series(LabelEncoder().fit_transform(demo.loc[:,['sex']]), index=demo.index, name='sex')\n",
    "\n",
    "control = pd.DataFrame({'sex':sex_coded, 'age':demo['interview_age']}) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "carit1 (504, 379)\n",
      "carit3 (504, 379)\n",
      "carit4 (504, 379)\n",
      "face1 (504, 379)\n",
      "face2 (504, 379)\n",
      "face3 (504, 379)\n",
      "face4 (504, 379)\n",
      "face5 (504, 379)\n",
      "face6 (504, 379)\n",
      "vism (504, 379)\n",
      "carit_FC (504, 71631)\n",
      "face_FC (504, 71631)\n",
      "vism_FC (504, 71631)\n",
      "cort (504, 148)\n",
      "surf (504, 148)\n",
      "subc (504, 19)\n",
      "VolBrain (504, 5)\n",
      "rest (504, 71631)\n"
     ]
    }
   ],
   "source": [
    "for key in features.keys():\n",
    "    print(key, features[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Leave-P-group out based on N-Fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import simplefilter\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    os.environ[\"PYTHONWARNINGS\"] = \"ignore\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "for COL in targ.columns:\n",
    "    #COL = 'nih_fluidcogcomp_unadjusted'  #the script adapted to be launched on table of target variables. To launch in that way you need to uncomment for loop and comment this row with col variable\n",
    "    y = targ[COL]\n",
    "\n",
    "    print(y.name)\n",
    "\n",
    "    ###make folder for outputs\n",
    "    nmf=path+'output_5cv_AllAdj_STDstackFeatures_'+y.name\n",
    "    os.mkdir(nmf)\n",
    "\n",
    "    i=0\n",
    "\n",
    "    group_kfold = GroupKFold(n_splits=5)\n",
    "    for train_index, test_index in group_kfold.split(demo, groups=demo['family_user_def_id']): \n",
    "\n",
    "        print(' ')\n",
    "        print('started to calculate the Fold #', i)\n",
    "        print(datetime.now())\n",
    "        print(' ')\n",
    "\n",
    "        ###create directory for specific Fold\n",
    "        os.mkdir(nmf+'/Fold_'+str(i)) \n",
    "        path_out = str(nmf+'/Fold_'+str(i))\n",
    "\n",
    "        ###Global indices\n",
    "        train_index = np.array(demo.iloc[train_index].index) #for training all models\n",
    "        test_index = np.array(demo.iloc[test_index].index) #for final test\n",
    "\n",
    "        ###Split global train_Gindex to local indices\n",
    "        #index_train, index_test = train_test_split(train_index, test_size=0.4, random_state=42)\n",
    "\n",
    "        ###Local indices\n",
    "        #index_train = np.array(sorted(index_train)) #for training modalities models\n",
    "        #index_test = np.array(sorted(index_test)) #for testing modalities and training RF\n",
    "\n",
    "\n",
    "        ### 1st level ################################################################################\n",
    "\n",
    "        #### Calculations of single ML models on index_train #################################### \n",
    "\n",
    "        print('start 1st level ', datetime.now())\n",
    "\n",
    "        #control for control table with sorting to train_index\n",
    "\n",
    "        #control y (target) \n",
    "        y_res1 , std_targ_y, std_targ_X, linreg_targ = control_features(y, control, train_index)\n",
    "        #= y.reindex(index=train_index)#\n",
    "\n",
    "        #control modalities\n",
    "        features_res1 = {}\n",
    "        std_feat_y_dct = {}\n",
    "        std_feat_X_dct = {}\n",
    "        linreg_feat_dct = {}\n",
    "        for key in features.keys():\n",
    "            print('controlling ', key, datetime.now())\n",
    "\n",
    "            mod_res, std_f_y, std_f_X, linreg_f = control_features(features[key], control, y_res1.index)\n",
    "\n",
    "            features_res1[key] = mod_res\n",
    "            std_feat_y_dct[key] = std_f_y\n",
    "            std_feat_X_dct[key] = std_f_X\n",
    "            linreg_feat_dct[key] = linreg_f\n",
    "\n",
    "        #save adjastment model\n",
    "        os.mkdir(path_out+'/adjustment_models')\n",
    "        #target models\n",
    "        joblib.dump(std_targ_y, (path_out+'/adjustment_models'+'/target_std_model_y.sav'))\n",
    "        joblib.dump(std_targ_X, (path_out+'/adjustment_models'+'/target_std_model_X.sav'))\n",
    "        joblib.dump(linreg_targ, (path_out+'/adjustment_models'+'/target_linreg.sav'))\n",
    "        #features model\n",
    "        joblib.dump(std_feat_y_dct, (path_out+'/adjustment_models'+'/features_std_model_y.sav'))\n",
    "        joblib.dump(std_feat_X_dct, (path_out+'/adjustment_models'+'/features_std_model_X.sav'))\n",
    "        joblib.dump(linreg_feat_dct, (path_out+'/adjustment_models'+'/features_linreg.sav'))\n",
    "\n",
    "\n",
    "        ###standartize before model and keep std models\n",
    "        #features\n",
    "        std_models_features = {}\n",
    "        for key in features_res1.keys():\n",
    "            print('standartize ', key, datetime.now())\n",
    "            std_model = StandardScaler()\n",
    "            std_model.fit(features_res1[key].values)\n",
    "            features_res1[key] = pd.DataFrame(std_model.transform(features_res1[key].values),\n",
    "                                              index=features_res1[key].index, \n",
    "                                              columns=features_res1[key].columns)\n",
    "            std_models_features[key] = std_model\n",
    "        #target\n",
    "        std_model_target = StandardScaler()\n",
    "        std_model_target.fit(y_res1.values.reshape(-1, 1))\n",
    "        y_res1 = pd.DataFrame(std_model_target.transform(y_res1.values.reshape(-1, 1)),\n",
    "                              index=y_res1.index)\n",
    "\n",
    "        #save \n",
    "        os.mkdir(path_out+'/standartization_models')\n",
    "        #target\n",
    "        joblib.dump(std_model_target,  (path_out+'/standartization_models'+'/target_std_model.sav'))\n",
    "        #features\n",
    "        joblib.dump(std_models_features,  (path_out+'/standartization_models'+'/features_std_model.sav'))\n",
    "\n",
    "\n",
    "        #save features table before PCA\n",
    "        y_res1.to_csv(path_out+'/target_y_train1.csv')\n",
    "        for key in features_res1.keys():\n",
    "            features_res1[key].to_csv(path_out+'/'+str(key)+'_train1.csv')\n",
    "\n",
    "\n",
    "        #PCA models to rest and task FC\n",
    "        PCA_models = {}\n",
    "        for key in ['rest', 'carit_FC', 'face_FC', 'vism_FC']:\n",
    "            print('reduction ', key, datetime.now())\n",
    "            model_PCA =  PCA(n_components=75, random_state=11)\n",
    "            model_PCA.fit(features_res1[key].values)\n",
    "            features_res1[key] = pd.DataFrame(model_PCA.transform(features_res1[key].values), \n",
    "                                              index=features_res1[key].index)\n",
    "            PCA_models[key] = model_PCA\n",
    "        #save PCA models\n",
    "        os.mkdir(path_out+'/PCA_models')\n",
    "        joblib.dump(PCA_models,  (path_out+'/PCA_models'+'/PCA_model.sav'))\n",
    "\n",
    "\n",
    "        #apply new std to PCA features again\n",
    "        std_PC_feature_models = {}\n",
    "        for key in ['rest', 'carit_FC', 'face_FC', 'vism_FC']:\n",
    "            print('standartize PC table ', key, datetime.now())\n",
    "            std_PC_model = StandardScaler()\n",
    "            std_PC_model.fit(features_res1[key].values)\n",
    "            features_res1[key] = pd.DataFrame(std_PC_model.transform(features_res1[key].values),\n",
    "                                              index=features_res1[key].index, \n",
    "                                              columns=features_res1[key].columns)\n",
    "            std_PC_feature_models[key] = std_PC_model\n",
    "            #save PCA tables\n",
    "            features_res1[key].to_csv(path_out+'/'+key+'_PCA75_train1.csv')\n",
    "        #save std PCA models\n",
    "        os.mkdir(path_out+'/PCA_standardization_models')\n",
    "        joblib.dump(std_PC_feature_models,  (path_out+'/PCA_standardization_models'+'/std_PCA_model.sav'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #Launch ElasticNet for all task(modalities) on index_train (1st level)\n",
    "\n",
    "        dict_tasks={}\n",
    "        dict_elnet_model={}\n",
    "        dict_ypred1={}\n",
    "\n",
    "        for key in list(features_res1.keys()):\n",
    "\n",
    "            print('start ', str(key), datetime.now())   #print start time of calculations\n",
    "\n",
    "            bpar1, bpar2, acc, mse, corr, model, y_pred1, mae = elnet(features_res1[key], y_res1) #ML\n",
    "            dict_tasks[key] = acc, mse, mae, corr, bpar1, bpar2 \n",
    "            dict_elnet_model[key] = model\n",
    "            dict_ypred1[key] = y_pred1\n",
    "        df_tasks = pd.DataFrame(dict_tasks, index=['best score r2', 'mse', 'mae','corr', 'best alpha', 'best l1_ratio'])\n",
    "        df_y_pred1 = pd.DataFrame(dict_ypred1, index=y_res1.index)\n",
    "\n",
    "\n",
    "        ###Save outputs from this step (models and all mod. perf.)\n",
    "\n",
    "        #models\n",
    "        for key in dict_elnet_model.keys():\n",
    "            joblib.dump(dict_elnet_model[key], (path_out+'/'+str(key)+'_elnet_model.sav'))\n",
    "\n",
    "        #model performance\n",
    "        df_tasks.to_csv(path_out+'/1level_train_perf_elnet.csv')\n",
    "\n",
    "        #list of first level targets (observed and predicted)\n",
    "        df_y_pred1.to_csv(path_out+'/1level_train_y_pred_singleML.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ### 2st level ################################################################################\n",
    "        print(' ')\n",
    "        print('start 2nd level ', datetime.now())\n",
    "\n",
    "        #### L2 Testing single ML models on index_test #############################################\n",
    "\n",
    "        #print('Checking single ML on train data ', datetime.now())\n",
    "\n",
    "        #control for control table with sorting to train_index\n",
    "\n",
    "        #control y (target) \n",
    "        y_res2 = re_control_features(y, control, train_index, \n",
    "                                     std_targ_y, std_targ_X, linreg_targ)\n",
    "        #y.reindex(index=train_index)#\n",
    "\n",
    "        #control modalities\n",
    "        features_res2 = {}\n",
    "        for key in features.keys():\n",
    "            print('controlling ', key, datetime.now())\n",
    "\n",
    "            features_res2[key] = re_control_features(features[key], control, y_res2.index, \n",
    "                                                     std_feat_y_dct[key], std_feat_X_dct[key], linreg_feat_dct[key])\n",
    "\n",
    "        ###standartize before model and keep std models\n",
    "        #features\n",
    "        for key in features_res2.keys():\n",
    "            print('standartize ', key, datetime.now())\n",
    "            features_res2[key] = pd.DataFrame(std_models_features[key].transform(features_res2[key].values),\n",
    "                                              index=features_res2[key].index, \n",
    "                                              columns=features_res2[key].columns)\n",
    "        #target\n",
    "        y_res2 = pd.DataFrame(std_model_target.transform(y_res2.values.reshape(-1, 1)),\n",
    "                              index=y_res2.index) \n",
    "\n",
    "        #save features table before PCA\n",
    "        y_res2.to_csv(path_out+'/target_y_train2.csv')\n",
    "        for key in features_res2.keys():\n",
    "            features_res2[key].to_csv(path_out+'/'+str(key)+'_train2.csv')            \n",
    "\n",
    "\n",
    "        #PCA models to rest and task FC\n",
    "        for key in ['rest', 'carit_FC', 'face_FC', 'vism_FC']:\n",
    "            print('reduction ', key, datetime.now())\n",
    "            features_res2[key] = pd.DataFrame(PCA_models[key].transform(features_res2[key].values),\n",
    "                                              index=features_res2[key].index)\n",
    "\n",
    "\n",
    "        #apply new std to PCA features again\n",
    "        for key in ['rest', 'carit_FC', 'face_FC', 'vism_FC']:\n",
    "            print('standartize PCA ', key, datetime.now())\n",
    "            features_res2[key] = pd.DataFrame(std_PC_feature_models[key].transform(features_res2[key].values),\n",
    "                                              index=features_res2[key].index, \n",
    "                                              columns=features_res2[key].columns)\n",
    "            #save std pc table\n",
    "            features_res2[key].to_csv(path_out+'/'+key+'_PCA75_train2.csv')\n",
    "\n",
    "\n",
    "        #apply trained single models ElasticNet to new data , index_test\n",
    "\n",
    "        dict_y_pred2={}\n",
    "        dict_y_pred2_per={}\n",
    "        for key in list(features_res2.keys()):\n",
    "            y_pred, y_real, ind_y, bacc, mse, corr, mae = reaply_ElNet(features_res2[key], y_res2, dict_elnet_model[key]) #ML\n",
    "            dict_y_pred2[key] = y_pred\n",
    "            dict_y_pred2_per[key] = bacc, mse, mae, corr\n",
    "\n",
    "        df_y_pred2 = pd.DataFrame(dict_y_pred2, index=ind_y)\n",
    "        df_y_pred2_per = pd.DataFrame(dict_y_pred2_per, index=['best score r2', 'mse', 'mae','corr'])\n",
    "\n",
    "\n",
    "        ###Save outputs from this step (models and all mod. perf.)\n",
    "\n",
    "        #model performance\n",
    "        df_y_pred2_per.to_csv(path_out+'/2level_test1_perf_elnet.csv')\n",
    "\n",
    "        #list of first level targets (observed and predicted)\n",
    "        df_y_pred2.to_csv(path_out+'/2level_test1_y_pred_singleML.csv')   \n",
    "\n",
    "\n",
    "\n",
    "        #### L2 Calculating stacked ML models on index_test #############################################\n",
    "\n",
    "        print('Calculating stacked ML on train data ', datetime.now())    \n",
    "\n",
    "\n",
    "        #identifying sets for several stacked models\n",
    "        set2 = ['carit1', 'carit3', 'carit4', 'face1', 'face2', 'face3', 'face4', 'face5', 'face6', 'vism']\n",
    "        set3 = ['cort', 'subc', 'surf', 'rest', 'VolBrain']\n",
    "\n",
    "        set4 = ['carit_FC', 'face_FC', 'vism_FC']\n",
    "        set5 = ['carit1', 'carit3', 'carit4', 'face1', 'face2', 'face3', 'face4', 'face5', 'face6', 'vism', 'carit_FC', 'face_FC', 'vism_FC']\n",
    "        set6 = ['carit1', 'carit3', 'carit4', 'face1', 'face2', 'face3', 'face4', 'face5', 'face6', 'vism', 'cort', 'subc', 'surf', 'rest', 'VolBrain']\n",
    "        set7 = ['carit_FC', 'face_FC', 'vism_FC', 'cort', 'subc', 'surf', 'rest', 'VolBrain']\n",
    "        set8 = ['carit_FC', 'face_FC', 'vism_FC', 'rest']\n",
    "\n",
    "        set1 = list(df_y_pred2.columns) #all existed modalities\n",
    "\n",
    "        #for presetet sets\n",
    "        dict_st_perf1={}\n",
    "        dict_st_models={}\n",
    "        dict_st_ypred1={}\n",
    "        dct_std_mod_for_stack = {} #\n",
    "        dct_std_tab_for_stack = {} #\n",
    "        dct_std_tab_before_for_stack = {} #\n",
    "\n",
    "        s=1\n",
    "        for set_n in [set1, set2, set3, set4, set5, set6, set7, set8]:\n",
    "            print('set '+str(s), datetime.now())\n",
    "\n",
    "            st_features = df_y_pred2.loc[:,set_n]\n",
    "            dct_std_tab_before_for_stack['set'+str(s)] = st_features #\n",
    "\n",
    "            stack_std_model = StandardScaler().fit(st_features.values) \n",
    "            dct_std_mod_for_stack['set'+str(s)] = stack_std_model #\n",
    "\n",
    "            std_st_features = pd.DataFrame(stack_std_model.transform(st_features.values), \n",
    "                                           index=st_features.index, columns=st_features.columns) \n",
    "            dct_std_tab_for_stack['set'+str(s)] = std_st_features #\n",
    "\n",
    "\n",
    "\n",
    "            bpar1, bpar2, acc, mse, corr, model, y_pred3, mae = elnet(std_st_features, y_res2) #ML\n",
    "\n",
    "            dict_st_perf1['set'+str(s)] = acc, mse, mae, corr, bpar1, bpar2 \n",
    "            dict_st_models['set'+str(s)] = model\n",
    "            dict_st_ypred1['set'+str(s)] = y_pred3\n",
    "            s+=1\n",
    "\n",
    "        df_st_perf1 = pd.DataFrame(dict_st_perf1, index=['best score r2', 'mse', 'mae','corr', 'best alpha', 'best l1_ratio'])\n",
    "        df_st_ypred1 = pd.DataFrame(dict_st_ypred1, index=y_res2.index)        \n",
    "\n",
    "        ###Save outputs from this step (models and all mod. perf.)\n",
    "\n",
    "        #models\n",
    "        for key in dict_st_models.keys():\n",
    "            joblib.dump(dict_st_models[key], (path_out+'/'+str(key)+'_stacked_model.sav'))\n",
    "        for key in dct_std_mod_for_stack.keys():\n",
    "            joblib.dump(dct_std_mod_for_stack[key], (path_out+'/'+str(key)+'_stacked_STD_model.sav'))\n",
    "\n",
    "        #performance and prediction\n",
    "        df_st_perf1.to_csv(path_out+'/2level_test1_perf_stacked.csv')\n",
    "        df_st_ypred1.to_csv(path_out+'/2level_test1_y_pred_stacked.csv')\n",
    "        for key in dct_std_tab_for_stack.keys():\n",
    "            dct_std_tab_for_stack[key].to_csv(path_out+'/2level_stack_y_feature_tab_STD.csv')\n",
    "            dct_std_tab_before_for_stack[key].to_csv(path_out+'/2level_stack_y_feature_tab_beforeSTD.csv')\n",
    "\n",
    "\n",
    "        ### 3rd level ################################################################################\n",
    "        print(' ')\n",
    "        print('start 3rd level ', datetime.now())\n",
    "\n",
    "\n",
    "        #### L3 Testing single ML models on test_index #############################################\n",
    "\n",
    "        print('Checking single ML on test data ', datetime.now())\n",
    "\n",
    "        #control for control table with sorting to test_index\n",
    "\n",
    "        #control y (target)\n",
    "        y_res3 =re_control_features(y, control, test_index, \n",
    "                                    std_targ_y, std_targ_X, linreg_targ)\n",
    "        # y.reindex(index=test_index)#\n",
    "\n",
    "        #control modalities\n",
    "        features_res3 = {}\n",
    "        for key in features.keys():\n",
    "            print('controlling ', key, datetime.now())\n",
    "\n",
    "            features_res3[key] = re_control_features(features[key], control, y_res3.index, \n",
    "                                                     std_feat_y_dct[key], std_feat_X_dct[key], linreg_feat_dct[key])\n",
    "\n",
    "        ###standartize before model and keep std models\n",
    "        #features\n",
    "        for key in features_res3.keys():\n",
    "            print('standartize ', key, datetime.now())\n",
    "            features_res3[key] = pd.DataFrame(std_models_features[key].transform(features_res3[key].values),\n",
    "                                              index=features_res3[key].index, \n",
    "                                              columns=features_res3[key].columns)\n",
    "        #target\n",
    "        y_res3 = pd.DataFrame(std_model_target.transform(y_res3.values.reshape(-1, 1)),\n",
    "                              index=y_res3.index) \n",
    "\n",
    "        #save features table before PCA\n",
    "        y_res3.to_csv(path_out+'/target_y_test.csv')\n",
    "        for key in features_res3.keys():\n",
    "            features_res3[key].to_csv(path_out+'/'+str(key)+'_test.csv')            \n",
    "\n",
    "\n",
    "        #PCA models to rest and task FC\n",
    "        for key in ['rest', 'carit_FC', 'face_FC', 'vism_FC']:\n",
    "            print('reduction ', key, datetime.now())\n",
    "            features_res3[key] = pd.DataFrame(PCA_models[key].transform(features_res3[key].values),\n",
    "                                              index=features_res3[key].index)\n",
    "\n",
    "\n",
    "        #apply new std to PCA features again\n",
    "        for key in ['rest', 'carit_FC', 'face_FC', 'vism_FC']:\n",
    "            print('standartize PCA ', key, datetime.now())\n",
    "            features_res3[key] = pd.DataFrame(std_PC_feature_models[key].transform(features_res3[key].values),\n",
    "                                              index=features_res3[key].index, \n",
    "                                              columns=features_res3[key].columns)\n",
    "            #save std pc table\n",
    "            features_res3[key].to_csv(path_out+'/'+key+'_PCA75_test.csv')\n",
    "\n",
    "\n",
    "\n",
    "        #apply trained single models ElasticNet to new data , test_index\n",
    "\n",
    "        dict_y_pred3={}\n",
    "        dict_y_pred3_per={}\n",
    "        for key in list(features_res3.keys()):\n",
    "            y_pred, y_real, ind_y, bacc, mse, corr, mae = reaply_ElNet(features_res3[key], y_res3, dict_elnet_model[key]) #ML\n",
    "            dict_y_pred3[key] = y_pred\n",
    "            dict_y_pred3_per[key] = bacc, mse, mae, corr\n",
    "\n",
    "        df_y_pred3 = pd.DataFrame(dict_y_pred3, index=ind_y)\n",
    "        df_y_pred3_per = pd.DataFrame(dict_y_pred3_per, index=['best score r2', 'mse', 'mae','corr'])\n",
    "\n",
    "\n",
    "        ###Save outputs from this step (models and all mod. perf.)\n",
    "\n",
    "        #model performance\n",
    "        df_y_pred3_per.to_csv(path_out+'/3level_test2_perf_elnet.csv')\n",
    "\n",
    "        #list of first level targets (observed and predicted)\n",
    "        df_y_pred3.to_csv(path_out+'/3level_test2_y_pred_singleML.csv')        \n",
    "\n",
    "\n",
    "        #### L3 Testing stacked ML models on test_index #############################################\n",
    "\n",
    "        print('Calculating stacked ML on test data ', datetime.now()) \n",
    "\n",
    "        #apply trained stacked models ElasticNet to new data , test_index\n",
    "\n",
    "        #for presetet sets\n",
    "        dict_st_perf2={}\n",
    "        dict_st_ypred2={}\n",
    "\n",
    "        dct_std3_tab_for_stack = {} #\n",
    "        dct_std3_tab_before_for_stack = {} #\n",
    "\n",
    "        s=1\n",
    "        for set_n in [set1, set2, set3, set4, set5, set6, set7, set8]:\n",
    "\n",
    "            ftrs = df_y_pred3.loc[:, set_n]\n",
    "            dct_std3_tab_before_for_stack['set'+str(s)] = ftrs\n",
    "\n",
    "            std_ftrs = pd.DataFrame(dct_std_mod_for_stack['set'+str(s)].transform(ftrs.values), \n",
    "                                    index=ftrs.index,columns=ftrs.columns)\n",
    "            dct_std3_tab_for_stack['set'+str(s)] = std_ftrs\n",
    "\n",
    "            y_pred, y_real, ind_y, bacc, mse, corr, mae = reaply_ElNet(std_ftrs, y_res3, dict_st_models[('set'+str(s))]) #ML\n",
    "            dict_st_ypred2[('set'+str(s))] = y_pred\n",
    "            dict_st_perf2[('set'+str(s))] = bacc, mse, mae, corr\n",
    "            s+=1\n",
    "\n",
    "        df_st_ypred2 = pd.DataFrame(dict_st_ypred2, index=ind_y)\n",
    "        df_st_perf2 = pd.DataFrame(dict_st_perf2, index=['best score r2', 'mse', 'mae','corr'])        \n",
    "\n",
    "        ###Save outputs from this step (models and all mod. perf.)\n",
    "\n",
    "        #performance and prediction\n",
    "        df_st_perf2.to_csv(path_out+'/3level_test2_perf_stacked.csv')\n",
    "        df_st_ypred2.to_csv(path_out+'/3level_test2_y_pred_stacked.csv') \n",
    "        for key in dct_std3_tab_for_stack.keys():\n",
    "            dct_std3_tab_for_stack[key].to_csv(path_out+'/3level_stack_y_feature_tab_STD.csv')\n",
    "            dct_std3_tab_before_for_stack[key].to_csv(path_out+'/3level_stack_y_feature_tab_beforeSTD.csv')\n",
    "\n",
    "        print(' ')\n",
    "        print('finished to calculate the Fold #', i)\n",
    "        print(datetime.now())\n",
    "\n",
    "        i+=1\n",
    "\n",
    "    print(' ')\n",
    "    print('finished the MODEL '+COL)\n",
    "    print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
