{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacked Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## IMPORTANT !\n",
    "\n",
    "# In the first order need to set the number of CPU \n",
    "# for calculation before launching (depends on computer's number of cores)\n",
    "n_jobs= 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import glob\n",
    "import joblib\n",
    "import warnings\n",
    "from datetime import date, datetime\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.model_selection import LeavePGroupsOut\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import resample\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "import scipy.stats as st\n",
    "\n",
    "from nilearn import image as nli\n",
    "from nilearn import plotting\n",
    "\n",
    "#from mne.viz import plot_connectivity_circle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def control_features(table_in, control, index): \n",
    "    #table_in should be a table of features, where rows - subjects, columns - features\n",
    "    \n",
    "    if len(table_in.values.shape) == 1: #for pd.Series # for target\n",
    "        \n",
    "        #shrink data to local train index\n",
    "        table_in = table_in.reindex(index = index)\n",
    "        control = control.reindex(index = index)\n",
    "        ind = table_in.index\n",
    "        \n",
    "        #loop\n",
    "        dct_table = {}\n",
    "        dct_lin_models ={}\n",
    "        dct_std_y_models ={}\n",
    "        \n",
    "        col='0'\n",
    "        \n",
    "        y = table_in #target, brain ROI\n",
    "        X = control  #features, like age, sex and/or movements\n",
    "\n",
    "        #Standartize target\n",
    "        std_model_y = StandardScaler()\n",
    "        std_model_y.fit(y.values.reshape(-1, 1))\n",
    "        y = std_model_y.transform(y.values.reshape(-1, 1))\n",
    "        \n",
    "        #reshaping data\n",
    "        if len(X.values.shape) == 1:\n",
    "            X = X.values.reshape(-1, 1)\n",
    "        else:\n",
    "            X = X.values\n",
    "        y = y.reshape(-1, 1).ravel()\n",
    "        \n",
    "        #Standartize X\n",
    "        std_model = StandardScaler()\n",
    "        std_model.fit(X)\n",
    "        X = std_model.transform(X)\n",
    "\n",
    "        #Fit to the training set\n",
    "        model = LinearRegression()\n",
    "        model.fit(X, y)\n",
    "        y_pred = model.predict(X)\n",
    "\n",
    "        y_res = y - y_pred\n",
    "\n",
    "        dct_table[col] = y_res\n",
    "        dct_lin_models[col] = model\n",
    "        dct_std_y_models[col] = std_model_y\n",
    "\n",
    "        df_table = pd.DataFrame(dct_table, index = ind)\n",
    "\n",
    "        \n",
    "    else:\n",
    "            \n",
    "        #shrink data to local train index\n",
    "        table_in = table_in.reindex(index = index)\n",
    "        control = control.reindex(index = index)\n",
    "        ind = table_in.index\n",
    "\n",
    "        #loop\n",
    "        dct_table = {}\n",
    "        dct_lin_models ={}\n",
    "        dct_std_y_models ={}\n",
    "        col_names = table_in.columns\n",
    "\n",
    "        for col in col_names:\n",
    "            y = table_in[col] #target, brain ROI\n",
    "            X = control  #features, like age, sex and/or movements\n",
    "            \n",
    "            #Standartize target\n",
    "            std_model_y = StandardScaler()\n",
    "            std_model_y.fit(y.values.reshape(-1, 1))\n",
    "            y = std_model_y.transform(y.values.reshape(-1, 1)) \n",
    "            \n",
    "            #reshaping data\n",
    "            if len(X.values.shape) == 1:\n",
    "                X = X.values.reshape(-1, 1)\n",
    "            else:\n",
    "                X = X.values\n",
    "            y = y.reshape(-1, 1).ravel()\n",
    "            \n",
    "            #Standartize X\n",
    "            std_model = StandardScaler()\n",
    "            std_model.fit(X)\n",
    "            X = std_model.transform(X)\n",
    "\n",
    "            #Fit to the training set\n",
    "            model = LinearRegression()\n",
    "            model.fit(X, y)\n",
    "            y_pred = model.predict(X)\n",
    "\n",
    "            y_res = y - y_pred\n",
    "\n",
    "            dct_table[col] = y_res\n",
    "            dct_lin_models[col] = model\n",
    "            dct_std_y_models[col] = std_model_y\n",
    "\n",
    "        df_table = pd.DataFrame(dct_table, index = ind)\n",
    "    \n",
    "    return df_table, dct_std_y_models, std_model, dct_lin_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def re_control_features(table_in, control, index, dct_std_y_models, std_model, dct_lin_models):\n",
    "    \n",
    "    if len(table_in.values.shape) == 1: #for pd.Series # for target\n",
    "        \n",
    "        #shrink data to local train index\n",
    "        table_in = table_in.reindex(index = index)\n",
    "        control = control.reindex(index = index)\n",
    "        ind = table_in.index\n",
    "        \n",
    "        #loop\n",
    "        dct_table = {}\n",
    "        \n",
    "        col='0'\n",
    "        \n",
    "        y = table_in #target, brain ROI\n",
    "        X = control  #features, like age, sex and/or movements\n",
    "        \n",
    "        #standartize y\n",
    "        y = dct_std_y_models[col].transform(y.values.reshape(-1, 1))\n",
    "        \n",
    "        #reshaping data\n",
    "        if len(X.values.shape) == 1:\n",
    "            X = X.values.reshape(-1, 1)\n",
    "        else:\n",
    "            X = X.values\n",
    "        y = y.reshape(-1, 1).ravel()\n",
    "\n",
    "        #Standartize X with previous std model\n",
    "        X = std_model.transform(X)\n",
    "\n",
    "        #Fit with previous LinReg model\n",
    "        y_pred =  dct_lin_models[col].predict(X)\n",
    "\n",
    "        y_res = y - y_pred\n",
    "\n",
    "        dct_table[col] = y_res\n",
    "\n",
    "        df_table = pd.DataFrame(dct_table, index = ind)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        #shrink data to local train index\n",
    "        table_in = table_in.reindex(index = index)\n",
    "        control = control.reindex(index = index)\n",
    "        ind = table_in.index\n",
    "\n",
    "        #loop\n",
    "        dct_table = {}\n",
    "        col_names = table_in.columns\n",
    "\n",
    "        for col in col_names:\n",
    "            y = table_in[col] #target, brain ROI\n",
    "            X = control  #features, like age, sex and/or movements\n",
    "\n",
    "            #standartize y\n",
    "            y = dct_std_y_models[col].transform(y.values.reshape(-1, 1))\n",
    "            \n",
    "            #reshaping data\n",
    "            if len(X.values.shape) == 1:\n",
    "                X = X.values.reshape(-1, 1)\n",
    "            else:\n",
    "                X = X.values\n",
    "            y = y.reshape(-1, 1).ravel()\n",
    "\n",
    "            #Standartize X with previous std model\n",
    "            X = std_model.transform(X)\n",
    "\n",
    "            #Fit with previous LinReg model\n",
    "            y_pred =  dct_lin_models[col].predict(X)\n",
    "\n",
    "            y_res = y - y_pred\n",
    "\n",
    "            dct_table[col] = y_res\n",
    "\n",
    "        df_table = pd.DataFrame(dct_table, index = ind)\n",
    "        \n",
    "    return df_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elnet(X, y):\n",
    "\n",
    "    #drop Nan in target and clean this subj from features\n",
    "    y = y.dropna()\n",
    "    X = X.loc[y.index,:]\n",
    "    ind_y = np.array(y.index)\n",
    "      \n",
    "    y_real=y\n",
    "    \n",
    "    #reshaping data\n",
    "    X = X.values\n",
    "    y = y.values.reshape(-1, 1).ravel()\n",
    "    \n",
    "    #fill Nan in X\n",
    "    #X = SimpleImputer(strategy='mean').fit_transform(X)\n",
    "    \n",
    "    #Standartize X\n",
    "    #X = StandardScaler().fit_transform(X)\n",
    "    \n",
    "    # Setup the pipeline steps:\n",
    "    steps = [('elasticnet', ElasticNet(random_state=42))]\n",
    "\n",
    "    # Create the pipeline: pipeline \n",
    "    pipeline = Pipeline(steps)\n",
    "\n",
    "    # Specify the hyperparameter space\n",
    "    parameters = {'elasticnet__alpha': np.logspace(-1, 2, 70),\n",
    "                  'elasticnet__l1_ratio':np.linspace(0,1,25)}\n",
    "\n",
    "    # Create the GridSearchCV object:\n",
    "    gm_cv = GridSearchCV(pipeline, parameters, cv=5, n_jobs=n_jobs)\n",
    "    \n",
    "    # Fit to the training set\n",
    "    gm_cv.fit(X, y)\n",
    "    \n",
    "    #predict new y\n",
    "    y_pred = gm_cv.predict(X)\n",
    "\n",
    "    # Compute and print the metrics\n",
    "    acc = gm_cv.best_score_\n",
    "    bpar = gm_cv.best_params_\n",
    "    model = gm_cv.best_estimator_\n",
    "    mse = mean_squared_error(y_real, y_pred)\n",
    "    mae = mean_absolute_error(y_real, y_pred)\n",
    "    corr, _ = pearsonr(np.array(y_real.values.reshape(-1, 1).ravel(), dtype=float), np.array(y_pred, dtype=float))\n",
    "            \n",
    "    return bpar['elasticnet__alpha'], bpar['elasticnet__l1_ratio'], acc, mse, corr, model, y_pred, mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reaply_ElNet(X, y, model):\n",
    "    # param should be pd.Series with indexes from model\n",
    "    \n",
    "    #drop Nan in target and clean this subj from features\n",
    "    y = y.dropna()\n",
    "    X = X.reindex(index =y.index)\n",
    "    ind_y = np.array(y.index)  # indexes as separate variable \n",
    "    \n",
    "    y_real = y\n",
    "\n",
    "    #reshaping data\n",
    "    X = X.values\n",
    "    y = y.values.reshape(-1, 1).ravel()\n",
    "    \n",
    "    #fill Nan in X\n",
    "    #X = SimpleImputer(strategy='mean').fit_transform(X)\n",
    "    \n",
    "    #Standartize X\n",
    "    #X = StandardScaler().fit_transform(X)\n",
    "    \n",
    "    #predict new y\n",
    "    y_pred = model.predict(X)\n",
    "    \n",
    "    # Compute and print the metrics\n",
    "    bacc = model.score(X, y)\n",
    "    mse = mean_squared_error(y_real, y_pred)\n",
    "    mae = mean_absolute_error(y_real, y_pred) \n",
    "    corr, _ = pearsonr(np.array(y_real.values.reshape(-1, 1).ravel(), dtype=float), np.array(y_pred, dtype=float))\n",
    "    \n",
    "    return y_pred, y_real, ind_y, bacc, mse, corr, mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path to the tables folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='/media/hcs-psy-narun/Alina/HCP_YA/MLtables_cope/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "#demography\n",
    "demo = pd.read_csv(path+'demographics_table.csv', index_col=0)\n",
    "\n",
    "#targets table\n",
    "targ = pd.read_csv(path+'cognition_table.csv', index_col=0)\n",
    "\n",
    "#features tables as dictionary\n",
    "features = {\n",
    "    'emo':pd.read_csv(path+'emo_table.csv', index_col=0),\n",
    "    'gam':pd.read_csv(path+'gam_table.csv', index_col=0),\n",
    "    'lan':pd.read_csv(path+'lan_table.csv', index_col=0),\n",
    "    'mot':pd.read_csv(path+'mot_table.csv', index_col=0),\n",
    "    'rel':pd.read_csv(path+'rel_table.csv', index_col=0),\n",
    "    'soc':pd.read_csv(path+'soc_table.csv', index_col=0),\n",
    "    'wm':pd.read_csv(path+'wm_table.csv', index_col=0),\n",
    "    \n",
    "    'gam_FC':pd.read_csv(path+'Task_FC_GAMBLING_group_z_full.csv', index_col=0),\n",
    "    'lan_FC':pd.read_csv(path+'Task_FC_LANGUAGE_group_z_full.csv', index_col=0),\n",
    "    'mot_FC':pd.read_csv(path+'Task_FC_MOTOR_group_z_full.csv', index_col=0),\n",
    "    'rel_FC':pd.read_csv(path+'Task_FC_RELATIONAL_group_z_full.csv', index_col=0),\n",
    "    'soc_FC':pd.read_csv(path+'Task_FC_SOCIAL_group_z_full.csv', index_col=0),\n",
    "    'wm_FC':pd.read_csv(path+'Task_FC_WM_group_z_full.csv', index_col=0),  \n",
    "    \n",
    "    'cort':pd.read_csv(path+'cort_table.csv', index_col=0),\n",
    "    'subc':pd.read_csv(path+'subc_table.csv', index_col=0),\n",
    "    'surf':pd.read_csv(path+'surf_table.csv', index_col=0),\n",
    "    'VolBrain':pd.read_csv(path+'VolBrain_table.csv', index_col=0),\n",
    "    'rest':pd.read_csv(path+'Rest_FC_group_z_full.csv', index_col=0),\n",
    "}\n",
    "\n",
    "\n",
    "#table with movements (mean relative displacement Movement_RelativeRMS_mean.txt)\n",
    "movements = pd.read_csv(path+'movement_table.csv', index_col=0)\n",
    "\n",
    "#create tables with 2 controling parameters: gender and age\n",
    "age_coded = pd.Series(LabelEncoder().fit_transform(demo.loc[:,['Gender']]), index=demo.index, name='Gender')\n",
    "control = pd.concat([age_coded, demo.loc[:, ['Age_in_Yrs']]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emo (882, 379)\n",
      "gam (882, 379)\n",
      "lan (882, 379)\n",
      "mot (882, 379)\n",
      "rel (882, 379)\n",
      "soc (882, 379)\n",
      "wm (882, 379)\n",
      "gam_FC (873, 71631)\n",
      "lan_FC (873, 71631)\n",
      "mot_FC (873, 71631)\n",
      "rel_FC (873, 71631)\n",
      "soc_FC (873, 71631)\n",
      "wm_FC (873, 71631)\n",
      "cort (882, 148)\n",
      "subc (882, 19)\n",
      "surf (882, 148)\n",
      "VolBrain (882, 5)\n",
      "rest (873, 71631)\n"
     ]
    }
   ],
   "source": [
    "for key in features.keys():\n",
    "    print(key, features[key].shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shrink tables to same subj numers\n",
    "yy = targ['CogTotalComp_Unadj'].dropna()\n",
    "\n",
    "demo = demo.reindex(index=yy.index)\n",
    "movements = movements.reindex(index=yy.index)\n",
    "control = control.reindex(index=yy.index)\n",
    "for key in features.keys():\n",
    "    features[key] = features[key].reindex(index=yy.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emo (873, 379)\n",
      "gam (873, 379)\n",
      "lan (873, 379)\n",
      "mot (873, 379)\n",
      "rel (873, 379)\n",
      "soc (873, 379)\n",
      "wm (873, 379)\n",
      "gam_FC (873, 71631)\n",
      "lan_FC (873, 71631)\n",
      "mot_FC (873, 71631)\n",
      "rel_FC (873, 71631)\n",
      "soc_FC (873, 71631)\n",
      "wm_FC (873, 71631)\n",
      "cort (873, 148)\n",
      "subc (873, 19)\n",
      "surf (873, 148)\n",
      "VolBrain (873, 5)\n",
      "rest (873, 71631)\n"
     ]
    }
   ],
   "source": [
    "for key in features.keys():\n",
    "    print(key, features[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#identifying sets for several stacked models\n",
    "#set1 - all\n",
    "#    set2 = ['emo', 'gam', 'lan', 'mot', 'rel', 'soc', 'wm']\n",
    "#    set3 = ['cort', 'subc', 'surf', 'rest', 'VolBrain']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Leave-P-group out based on 8-Fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CogTotalComp_Unadj\n",
      " \n",
      "started to calculate the Fold # 0\n",
      "2023-10-06 19:35:58.112195\n",
      " \n",
      "start 1st level  2023-10-06 19:35:58.124789\n",
      "controlling  emo 2023-10-06 19:35:58.199422\n",
      "controlling  gam 2023-10-06 19:35:58.709954\n",
      "controlling  lan 2023-10-06 19:35:59.102838\n",
      "controlling  mot 2023-10-06 19:35:59.564172\n",
      "controlling  rel 2023-10-06 19:35:59.973859\n",
      "controlling  soc 2023-10-06 19:36:00.374468\n",
      "controlling  wm 2023-10-06 19:36:00.768078\n",
      "controlling  gam_FC 2023-10-06 19:36:01.176895\n",
      "controlling  lan_FC 2023-10-06 19:37:12.683673\n",
      "controlling  mot_FC 2023-10-06 19:38:29.347135\n",
      "controlling  rel_FC 2023-10-06 19:39:42.058255\n",
      "controlling  soc_FC 2023-10-06 19:40:57.367743\n",
      "controlling  wm_FC 2023-10-06 19:42:10.580873\n",
      "controlling  cort 2023-10-06 19:43:21.378288\n",
      "controlling  subc 2023-10-06 19:43:21.635853\n",
      "controlling  surf 2023-10-06 19:43:21.659785\n",
      "controlling  VolBrain 2023-10-06 19:43:21.839354\n",
      "controlling  rest 2023-10-06 19:43:21.846321\n",
      "standartize  emo 2023-10-06 19:47:53.465759\n",
      "standartize  gam 2023-10-06 19:47:53.472055\n",
      "standartize  lan 2023-10-06 19:47:53.474955\n",
      "standartize  mot 2023-10-06 19:47:53.477797\n",
      "standartize  rel 2023-10-06 19:47:53.480650\n",
      "standartize  soc 2023-10-06 19:47:53.483471\n",
      "standartize  wm 2023-10-06 19:47:53.486369\n",
      "standartize  gam_FC 2023-10-06 19:47:53.489293\n",
      "standartize  lan_FC 2023-10-06 19:47:54.089279\n",
      "standartize  mot_FC 2023-10-06 19:47:54.678556\n",
      "standartize  rel_FC 2023-10-06 19:47:55.269365\n",
      "standartize  soc_FC 2023-10-06 19:47:55.859311\n",
      "standartize  wm_FC 2023-10-06 19:47:56.449574\n",
      "standartize  cort 2023-10-06 19:47:57.038822\n",
      "standartize  subc 2023-10-06 19:47:57.040957\n",
      "standartize  surf 2023-10-06 19:47:57.041762\n",
      "standartize  VolBrain 2023-10-06 19:47:57.043217\n",
      "standartize  rest 2023-10-06 19:47:57.043928\n",
      "reduction  rest 2023-10-06 20:08:22.563525\n",
      "reduction  gam_FC 2023-10-06 20:08:24.351201\n",
      "reduction  lan_FC 2023-10-06 20:08:25.945010\n",
      "reduction  mot_FC 2023-10-06 20:08:27.566799\n",
      "reduction  rel_FC 2023-10-06 20:08:29.204063\n",
      "reduction  soc_FC 2023-10-06 20:08:30.840780\n",
      "reduction  wm_FC 2023-10-06 20:08:32.507178\n",
      "standartize PC table  rest 2023-10-06 20:08:36.329877\n",
      "standartize PC table  gam_FC 2023-10-06 20:08:36.530521\n",
      "standartize PC table  lan_FC 2023-10-06 20:08:36.796768\n",
      "standartize PC table  mot_FC 2023-10-06 20:08:36.954499\n",
      "standartize PC table  rel_FC 2023-10-06 20:08:37.046403\n",
      "standartize PC table  soc_FC 2023-10-06 20:08:37.132645\n",
      "standartize PC table  wm_FC 2023-10-06 20:08:37.246875\n",
      "start  emo 2023-10-06 20:08:37.647636\n",
      "start  gam 2023-10-06 20:08:45.776792\n",
      "start  lan 2023-10-06 20:08:52.965441\n",
      "start  mot 2023-10-06 20:09:00.059865\n",
      "start  rel 2023-10-06 20:09:06.595809\n",
      "start  soc 2023-10-06 20:09:13.046494\n",
      "start  wm 2023-10-06 20:09:20.084656\n",
      "start  gam_FC 2023-10-06 20:09:27.040451\n",
      "start  lan_FC 2023-10-06 20:09:33.034676\n",
      "start  mot_FC 2023-10-06 20:09:38.329918\n",
      "start  rel_FC 2023-10-06 20:09:45.162831\n",
      "start  soc_FC 2023-10-06 20:09:51.425326\n",
      "start  wm_FC 2023-10-06 20:09:57.703067\n",
      "start  cort 2023-10-06 20:10:02.771795\n",
      "start  subc 2023-10-06 20:10:10.359331\n",
      "start  surf 2023-10-06 20:10:15.935442\n",
      "start  VolBrain 2023-10-06 20:10:22.907129\n",
      "start  rest 2023-10-06 20:10:28.464888\n",
      " \n",
      "start 2nd level  2023-10-06 20:10:35.768334\n",
      "Checking single ML on test1 data  2023-10-06 20:10:35.768422\n",
      "controlling  emo 2023-10-06 20:10:35.770701\n",
      "controlling  gam 2023-10-06 20:10:35.875007\n",
      "controlling  lan 2023-10-06 20:10:35.969176\n",
      "controlling  mot 2023-10-06 20:10:36.064265\n",
      "controlling  rel 2023-10-06 20:10:36.158826\n",
      "controlling  soc 2023-10-06 20:10:36.253267\n",
      "controlling  wm 2023-10-06 20:10:36.348071\n",
      "controlling  gam_FC 2023-10-06 20:10:36.442119\n",
      "controlling  lan_FC 2023-10-06 20:10:51.062045\n",
      "controlling  mot_FC 2023-10-06 20:11:06.181216\n",
      "controlling  rel_FC 2023-10-06 20:11:20.727345\n",
      "controlling  soc_FC 2023-10-06 20:11:35.315662\n",
      "controlling  wm_FC 2023-10-06 20:11:49.765492\n",
      "controlling  cort 2023-10-06 20:12:04.166940\n",
      "controlling  subc 2023-10-06 20:12:04.217112\n",
      "controlling  surf 2023-10-06 20:12:04.221252\n",
      "controlling  VolBrain 2023-10-06 20:12:04.247740\n",
      "controlling  rest 2023-10-06 20:12:04.249292\n",
      "standartize  emo 2023-10-06 20:12:19.201993\n",
      "standartize  gam 2023-10-06 20:12:19.209225\n",
      "standartize  lan 2023-10-06 20:12:19.216388\n",
      "standartize  mot 2023-10-06 20:12:19.223164\n",
      "standartize  rel 2023-10-06 20:12:19.229534\n",
      "standartize  soc 2023-10-06 20:12:19.230858\n",
      "standartize  wm 2023-10-06 20:12:19.232105\n",
      "standartize  gam_FC 2023-10-06 20:12:19.233354\n",
      "standartize  lan_FC 2023-10-06 20:12:19.436262\n",
      "standartize  mot_FC 2023-10-06 20:12:19.638721\n",
      "standartize  rel_FC 2023-10-06 20:12:19.834671\n",
      "standartize  soc_FC 2023-10-06 20:12:20.027258\n",
      "standartize  wm_FC 2023-10-06 20:12:20.219398\n",
      "standartize  cort 2023-10-06 20:12:20.407566\n",
      "standartize  subc 2023-10-06 20:12:20.408735\n",
      "standartize  surf 2023-10-06 20:12:20.409338\n",
      "standartize  VolBrain 2023-10-06 20:12:20.410092\n",
      "standartize  rest 2023-10-06 20:12:20.410584\n",
      "reduction  rest 2023-10-06 20:31:45.633569\n",
      "reduction  gam_FC 2023-10-06 20:31:45.808520\n",
      "reduction  lan_FC 2023-10-06 20:31:45.970142\n",
      "reduction  mot_FC 2023-10-06 20:31:46.142176\n",
      "reduction  rel_FC 2023-10-06 20:31:46.299127\n",
      "reduction  soc_FC 2023-10-06 20:31:46.454465\n",
      "reduction  wm_FC 2023-10-06 20:31:46.605991\n",
      "standartize PCA  rest 2023-10-06 20:31:46.757651\n",
      "standartize PCA  gam_FC 2023-10-06 20:31:46.866214\n",
      "standartize PCA  lan_FC 2023-10-06 20:31:46.976190\n",
      "standartize PCA  mot_FC 2023-10-06 20:31:47.095033\n",
      "standartize PCA  rel_FC 2023-10-06 20:31:47.188344\n",
      "standartize PCA  soc_FC 2023-10-06 20:31:47.315751\n",
      "standartize PCA  wm_FC 2023-10-06 20:31:47.440053\n",
      "Calculating stacked ML on test1 data  2023-10-06 20:31:47.807081\n",
      "set 1 2023-10-06 20:31:47.807537\n",
      "set 2 2023-10-06 20:31:54.869537\n",
      "set 3 2023-10-06 20:31:59.320722\n",
      "set 4 2023-10-06 20:32:04.611977\n",
      "set 5 2023-10-06 20:32:10.197754\n",
      "set 6 2023-10-06 20:32:16.063075\n",
      "set 7 2023-10-06 20:32:22.149681\n",
      "set 8 2023-10-06 20:32:28.754259\n",
      " \n",
      "start 3rd level  2023-10-06 20:32:35.343315\n",
      "Checking single ML on test2 data  2023-10-06 20:32:35.343369\n",
      "controlling  emo 2023-10-06 20:32:35.345227\n",
      "controlling  gam 2023-10-06 20:32:35.449433\n",
      "controlling  lan 2023-10-06 20:32:35.547569\n",
      "controlling  mot 2023-10-06 20:32:35.646609\n",
      "controlling  rel 2023-10-06 20:32:35.745421\n",
      "controlling  soc 2023-10-06 20:32:35.844481\n",
      "controlling  wm 2023-10-06 20:32:35.943131\n",
      "controlling  gam_FC 2023-10-06 20:32:36.041266\n",
      "controlling  lan_FC 2023-10-06 20:32:51.865221\n",
      "controlling  mot_FC 2023-10-06 20:33:06.824699\n",
      "controlling  rel_FC 2023-10-06 20:33:21.928267\n",
      "controlling  soc_FC 2023-10-06 20:33:36.859789\n",
      "controlling  wm_FC 2023-10-06 20:33:51.869577\n",
      "controlling  cort 2023-10-06 20:34:06.472414\n",
      "controlling  subc 2023-10-06 20:34:06.522221\n",
      "controlling  surf 2023-10-06 20:34:06.526384\n",
      "controlling  VolBrain 2023-10-06 20:34:06.555471\n",
      "controlling  rest 2023-10-06 20:34:06.557128\n",
      "standartize  emo 2023-10-06 20:34:21.086102\n",
      "standartize  gam 2023-10-06 20:34:21.092843\n",
      "standartize  lan 2023-10-06 20:34:21.100370\n",
      "standartize  mot 2023-10-06 20:34:21.107660\n",
      "standartize  rel 2023-10-06 20:34:21.113912\n",
      "standartize  soc 2023-10-06 20:34:21.115245\n",
      "standartize  wm 2023-10-06 20:34:21.116523\n",
      "standartize  gam_FC 2023-10-06 20:34:21.117774\n",
      "standartize  lan_FC 2023-10-06 20:34:21.314434\n",
      "standartize  mot_FC 2023-10-06 20:34:21.509646\n",
      "standartize  rel_FC 2023-10-06 20:34:21.698658\n",
      "standartize  soc_FC 2023-10-06 20:34:21.889007\n",
      "standartize  wm_FC 2023-10-06 20:34:22.077821\n",
      "standartize  cort 2023-10-06 20:34:22.266880\n",
      "standartize  subc 2023-10-06 20:34:22.267953\n",
      "standartize  surf 2023-10-06 20:34:22.268578\n",
      "standartize  VolBrain 2023-10-06 20:34:22.269557\n",
      "standartize  rest 2023-10-06 20:34:22.270207\n",
      "reduction  rest 2023-10-06 20:53:03.625748\n",
      "reduction  gam_FC 2023-10-06 20:53:03.852654\n",
      "reduction  lan_FC 2023-10-06 20:53:04.028070\n",
      "reduction  mot_FC 2023-10-06 20:53:04.219651\n",
      "reduction  rel_FC 2023-10-06 20:53:04.403301\n",
      "reduction  soc_FC 2023-10-06 20:53:04.557041\n",
      "reduction  wm_FC 2023-10-06 20:53:04.710708\n",
      "standartize PCA  rest 2023-10-06 20:53:04.860876\n",
      "standartize PCA  gam_FC 2023-10-06 20:53:05.018803\n",
      "standartize PCA  lan_FC 2023-10-06 20:53:05.227713\n",
      "standartize PCA  mot_FC 2023-10-06 20:53:05.316773\n",
      "standartize PCA  rel_FC 2023-10-06 20:53:05.407650\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standartize PCA  soc_FC 2023-10-06 20:53:05.493158\n",
      "standartize PCA  wm_FC 2023-10-06 20:53:05.582963\n",
      "Calculating stacked ML on test2 data  2023-10-06 20:53:05.812450\n",
      " \n",
      "finished to calculate the Fold # 0\n",
      "2023-10-06 20:53:06.794970\n",
      " \n",
      "finished the MODEL CogTotalComp_Unadj\n",
      "2023-10-06 20:53:06.795103\n"
     ]
    }
   ],
   "source": [
    "from warnings import simplefilter\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "simplefilter(\"ignore\", category=ConvergenceWarning)\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    os.environ[\"PYTHONWARNINGS\"] = \"ignore\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "#for COL in targ.columns:\n",
    "COL = 'CogTotalComp_Unadj'  #the script adapted to be launched on table of target variables. To launch in that way you need to uncomment for loop and comment this row with col variable\n",
    "y = yy\n",
    "\n",
    "print(y.name)\n",
    "\n",
    "###make folder for outputs\n",
    "nmf=path+'output_new_enh_newADJ_SHAP_newREST_'+y.name\n",
    "os.mkdir(nmf)\n",
    "\n",
    "i=0\n",
    "\n",
    "#group_kfold = GroupKFold(n_splits=8)\n",
    "#for train_index, test_index in group_kfold.split(demo, groups=demo['Family_ID']): \n",
    "\n",
    "print(' ')\n",
    "print('started to calculate the Fold #', i)\n",
    "print(datetime.now())\n",
    "print(' ')\n",
    "\n",
    "###create directory for specific Fold\n",
    "os.mkdir(nmf+'/Fold_'+str(i)) \n",
    "path_out = str(nmf+'/Fold_'+str(i))\n",
    "\n",
    "###Global indices\n",
    "train_index = y.index\n",
    "test_index = y.index\n",
    "\n",
    "###Split global train_Gindex to local indices\n",
    "#index_train, index_test = train_test_split(train_index, test_size=0.4, random_state=42)\n",
    "\n",
    "###Local indices\n",
    "#index_train = np.array(sorted(index_train)) #for training modalities models\n",
    "#index_test = np.array(sorted(index_test)) #for testing modalities and training RF\n",
    "\n",
    "\n",
    "### 1st level ################################################################################\n",
    "\n",
    "#### Calculations of single ML models on train_index #################################### \n",
    "\n",
    "print('start 1st level ', datetime.now())\n",
    "\n",
    "#control for control table with sorting to train_index\n",
    "\n",
    "#control y (target) \n",
    "y_res1, std_targ_y, std_targ_X, linreg_targ = control_features(y, control, train_index)\n",
    "\n",
    "\n",
    "#control modalities\n",
    "features_res1 = {}\n",
    "std_feat_y_dct = {}\n",
    "std_feat_X_dct = {}\n",
    "linreg_feat_dct = {}\n",
    "for key in features.keys():\n",
    "    print('controlling ', key, datetime.now())\n",
    "\n",
    "    #mod_res, std_f_y, std_f_X, linreg_f = control_features(features[key], control, y_res1.index)\n",
    "    if key in ['emo', 'gam', 'lan', 'mot', 'rel', 'soc', 'wm']:\n",
    "        control_t = pd.concat([control, movements[key]], axis=1)\n",
    "        mod_res, std_f_y, std_f_X, linreg_f = control_features(features[key], control_t, y_res1.index)\n",
    "    else:\n",
    "        mod_res, std_f_y, std_f_X, linreg_f = control_features(features[key], control, y_res1.index)\n",
    "\n",
    "    features_res1[key] = mod_res\n",
    "    std_feat_y_dct[key] = std_f_y\n",
    "    std_feat_X_dct[key] = std_f_X\n",
    "    linreg_feat_dct[key] = linreg_f\n",
    "\n",
    "#save adjastment model\n",
    "os.mkdir(path_out+'/adjustment_models')\n",
    "#target models\n",
    "joblib.dump(std_targ_y, (path_out+'/adjustment_models'+'/target_std_model_y.sav'))\n",
    "joblib.dump(std_targ_X, (path_out+'/adjustment_models'+'/target_std_model_X.sav'))\n",
    "joblib.dump(linreg_targ, (path_out+'/adjustment_models'+'/target_linreg.sav'))\n",
    "#features model\n",
    "joblib.dump(std_feat_y_dct, (path_out+'/adjustment_models'+'/features_std_model_y.sav'))\n",
    "joblib.dump(std_feat_X_dct, (path_out+'/adjustment_models'+'/features_std_model_X.sav'))\n",
    "joblib.dump(linreg_feat_dct, (path_out+'/adjustment_models'+'/features_linreg.sav'))\n",
    "\n",
    "\n",
    "###standartize before model and keep std models\n",
    "#features\n",
    "std_models_features = {}\n",
    "for key in features_res1.keys():\n",
    "    print('standartize ', key, datetime.now())\n",
    "    std_model = StandardScaler()\n",
    "    std_model.fit(features_res1[key].values)\n",
    "    features_res1[key] = pd.DataFrame(std_model.transform(features_res1[key].values),\n",
    "                                      index=features_res1[key].index, \n",
    "                                      columns=features_res1[key].columns)\n",
    "    std_models_features[key] = std_model\n",
    "#target\n",
    "std_model_target = StandardScaler()\n",
    "std_model_target.fit(y_res1.values.reshape(-1, 1))\n",
    "y_res1 = pd.DataFrame(std_model_target.transform(y_res1.values.reshape(-1, 1)),\n",
    "                      index=y_res1.index)\n",
    "\n",
    "#save \n",
    "os.mkdir(path_out+'/standartization_models')\n",
    "#target\n",
    "joblib.dump(std_model_target,  (path_out+'/standartization_models'+'/target_std_model.sav'))\n",
    "#features\n",
    "joblib.dump(std_models_features,  (path_out+'/standartization_models'+'/features_std_model.sav'))\n",
    "\n",
    "\n",
    "#save features table before PCA\n",
    "y_res1.to_csv(path_out+'/target_y_train1.csv')\n",
    "for key in features_res1.keys():\n",
    "    features_res1[key].to_csv(path_out+'/'+str(key)+'_train1.csv')\n",
    "\n",
    "\n",
    "#PCA models to rest and task FC\n",
    "PCA_models = {}\n",
    "for key in ['rest', 'gam_FC', 'lan_FC', 'mot_FC', 'rel_FC', 'soc_FC', 'wm_FC']:\n",
    "    print('reduction ', key, datetime.now())\n",
    "    model_PCA =  PCA(n_components=75, random_state=11)\n",
    "    model_PCA.fit(features_res1[key].values)\n",
    "    features_res1[key] = pd.DataFrame(model_PCA.transform(features_res1[key].values), \n",
    "                                      index=features_res1[key].index)\n",
    "    PCA_models[key] = model_PCA\n",
    "#save PCA models\n",
    "os.mkdir(path_out+'/PCA_models')\n",
    "joblib.dump(PCA_models,  (path_out+'/PCA_models'+'/PCA_model.sav'))\n",
    "\n",
    "\n",
    "#apply new std to PCA features again\n",
    "std_PC_feature_models = {}\n",
    "for key in ['rest', 'gam_FC', 'lan_FC', 'mot_FC', 'rel_FC', 'soc_FC', 'wm_FC']:\n",
    "    print('standartize PC table ', key, datetime.now())\n",
    "    std_PC_model = StandardScaler()\n",
    "    std_PC_model.fit(features_res1[key].values)\n",
    "    features_res1[key] = pd.DataFrame(std_PC_model.transform(features_res1[key].values),\n",
    "                                      index=features_res1[key].index, \n",
    "                                      columns=features_res1[key].columns)\n",
    "    std_PC_feature_models[key] = std_PC_model\n",
    "    #save PCA tables\n",
    "    features_res1[key].to_csv(path_out+'/'+key+'_PCA75_train1.csv')\n",
    "#save std PCA models\n",
    "os.mkdir(path_out+'/PCA_standardization_models')\n",
    "joblib.dump(std_PC_feature_models,  (path_out+'/PCA_standardization_models'+'/std_PCA_model.sav'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Launch ElasticNet for all task(modalities) on index_train (1st level)\n",
    "\n",
    "dict_tasks={}\n",
    "dict_elnet_model={}\n",
    "dict_ypred1={}\n",
    "\n",
    "for key in list(features_res1.keys()):\n",
    "\n",
    "    print('start ', str(key), datetime.now())   #print start time of calculations\n",
    "\n",
    "    bpar1, bpar2, acc, mse, corr, model, y_pred1, mae = elnet(features_res1[key], y_res1) #ML\n",
    "    dict_tasks[key] = acc, mse, mae, corr, bpar1, bpar2 \n",
    "    dict_elnet_model[key] = model\n",
    "    dict_ypred1[key] = y_pred1\n",
    "df_tasks = pd.DataFrame(dict_tasks, index=['best score r2', 'mse', 'mae','corr', 'best alpha', 'best l1_ratio'])\n",
    "df_y_pred1 = pd.DataFrame(dict_ypred1, index=y_res1.index)\n",
    "\n",
    "\n",
    "###Save outputs from this step (models and all mod. perf.)\n",
    "\n",
    "#models\n",
    "for key in dict_elnet_model.keys():\n",
    "    joblib.dump(dict_elnet_model[key], (path_out+'/'+str(key)+'_elnet_model.sav'))\n",
    "\n",
    "#model performance\n",
    "df_tasks.to_csv(path_out+'/1level_train_perf_elnet.csv')\n",
    "\n",
    "#list of first level targets (observed and predicted)\n",
    "df_y_pred1.to_csv(path_out+'/1level_train_y_pred_singleML.csv')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 2st level ################################################################################\n",
    "print(' ')\n",
    "print('start 2nd level ', datetime.now())\n",
    "\n",
    "#### L2 Testing single ML models on train_index #############################################\n",
    "\n",
    "print('Checking single ML on train data ', datetime.now())\n",
    "\n",
    "#control for control table with sorting to train_index\n",
    "\n",
    "#control y (target)\n",
    "y_res2 = re_control_features(y, control, train_index, \n",
    "                             std_targ_y, std_targ_X, linreg_targ)\n",
    "\n",
    "#control modalities\n",
    "features_res2 = {}\n",
    "for key in features.keys():\n",
    "    print('controlling ', key, datetime.now())\n",
    "\n",
    "    #features_res2[key] = re_control_features(features[key], control, y_res2.index, \n",
    "    #                                         std_feat_y_dct[key], std_feat_X_dct[key], linreg_feat_dct[key])\n",
    "    if key in ['emo', 'gam', 'lan', 'mot', 'rel', 'soc', 'wm']:\n",
    "        control_t = pd.concat([control, movements[key]], axis=1)\n",
    "        features_res2[key] = re_control_features(features[key], control_t, y_res2.index, \n",
    "                                             std_feat_y_dct[key], std_feat_X_dct[key], linreg_feat_dct[key])\n",
    "    else:\n",
    "        features_res2[key] = re_control_features(features[key], control, y_res2.index, \n",
    "                                             std_feat_y_dct[key], std_feat_X_dct[key], linreg_feat_dct[key])\n",
    "\n",
    "###standartize before model and keep std models\n",
    "#features\n",
    "for key in features_res2.keys():\n",
    "    print('standartize ', key, datetime.now())\n",
    "    features_res2[key] = pd.DataFrame(std_models_features[key].transform(features_res2[key].values),\n",
    "                                      index=features_res2[key].index, \n",
    "                                      columns=features_res2[key].columns)\n",
    "#target\n",
    "y_res2 = pd.DataFrame(std_model_target.transform(y_res2.values.reshape(-1, 1)),\n",
    "                      index=y_res2.index) \n",
    "\n",
    "#save features table before PCA\n",
    "y_res2.to_csv(path_out+'/target_y_train2.csv')\n",
    "for key in features_res2.keys():\n",
    "    features_res2[key].to_csv(path_out+'/'+str(key)+'_train2.csv')            \n",
    "\n",
    "\n",
    "#PCA models to rest and task FC\n",
    "for key in ['rest', 'gam_FC', 'lan_FC', 'mot_FC', 'rel_FC', 'soc_FC', 'wm_FC']:\n",
    "    print('reduction ', key, datetime.now())\n",
    "    features_res2[key] = pd.DataFrame(PCA_models[key].transform(features_res2[key].values), \n",
    "                              index=features_res2[key].index)\n",
    "\n",
    "\n",
    "#apply new std to PCA features again\n",
    "for key in ['rest', 'gam_FC', 'lan_FC', 'mot_FC', 'rel_FC', 'soc_FC', 'wm_FC']:\n",
    "    print('standartize PCA ', key, datetime.now())\n",
    "    features_res2[key] = pd.DataFrame(std_PC_feature_models[key].transform(features_res2[key].values),\n",
    "                                      index=features_res2[key].index, \n",
    "                                      columns=features_res2[key].columns)\n",
    "    #save std pc table\n",
    "    features_res2[key].to_csv(path_out+'/'+key+'_PCA75_train2.csv')\n",
    "\n",
    "\n",
    "#apply trained single models ElasticNet to new data , index_test\n",
    "\n",
    "dict_y_pred2={}\n",
    "dict_y_pred2_per={}\n",
    "for key in list(features_res2.keys()):\n",
    "    y_pred, y_real, ind_y, bacc, mse, corr, mae = reaply_ElNet(features_res2[key], y_res2, dict_elnet_model[key]) #ML\n",
    "    dict_y_pred2[key] = y_pred\n",
    "    dict_y_pred2_per[key] = bacc, mse, mae, corr\n",
    "\n",
    "df_y_pred2 = pd.DataFrame(dict_y_pred2, index=ind_y)\n",
    "df_y_pred2_per = pd.DataFrame(dict_y_pred2_per, index=['best score r2', 'mse', 'mae','corr'])\n",
    "\n",
    "\n",
    "###Save outputs from this step (models and all mod. perf.)\n",
    "\n",
    "#model performance\n",
    "df_y_pred2_per.to_csv(path_out+'/2level_test1_perf_elnet.csv')\n",
    "\n",
    "#list of first level targets (observed and predicted)\n",
    "df_y_pred2.to_csv(path_out+'/2level_test1_y_pred_singleML.csv')   \n",
    "\n",
    "\n",
    "\n",
    "#### L2 Calculating stacked ML models on index_test #############################################\n",
    "\n",
    "print('Calculating stacked ML on train data ', datetime.now())    \n",
    "\n",
    "\n",
    "#identifying sets for several stacked models\n",
    "set2 = ['emo', 'gam', 'lan', 'mot', 'rel', 'soc', 'wm']\n",
    "set3 = ['cort', 'subc', 'surf', 'rest', 'VolBrain']\n",
    "\n",
    "set4 = ['gam_FC', 'lan_FC', 'mot_FC', 'rel_FC', 'soc_FC', 'wm_FC']\n",
    "set5 = ['emo', 'gam', 'lan', 'mot', 'rel', 'soc', 'wm', 'gam_FC', 'lan_FC', 'mot_FC', 'rel_FC', 'soc_FC', 'wm_FC']\n",
    "set6 = ['emo', 'gam', 'lan', 'mot', 'rel', 'soc', 'wm', 'cort', 'subc', 'surf', 'rest', 'VolBrain']\n",
    "set7 = ['gam_FC', 'lan_FC', 'mot_FC', 'rel_FC', 'soc_FC', 'wm_FC', 'cort', 'subc', 'surf', 'rest', 'VolBrain']\n",
    "set8 = ['gam_FC', 'lan_FC', 'mot_FC', 'rel_FC', 'soc_FC', 'wm_FC', 'rest']\n",
    "\n",
    "set1 = list(df_y_pred2.columns) #all existed modalities\n",
    "\n",
    "#for presetet sets\n",
    "dict_st_perf1={}\n",
    "dict_st_models={}\n",
    "dict_st_ypred1={}\n",
    "dct_std_mod_for_stack = {} #\n",
    "dct_std_tab_for_stack = {} #\n",
    "dct_std_tab_before_for_stack = {} #\n",
    "\n",
    "s=1\n",
    "for set_n in [set1, set2, set3, set4, set5, set6, set7, set8]:\n",
    "    print('set '+str(s), datetime.now())\n",
    "\n",
    "    st_features = df_y_pred2.loc[:,set_n]\n",
    "    dct_std_tab_before_for_stack['set'+str(s)] = st_features #\n",
    "\n",
    "    stack_std_model = StandardScaler().fit(st_features.values) \n",
    "    dct_std_mod_for_stack['set'+str(s)] = stack_std_model #\n",
    "\n",
    "    std_st_features = pd.DataFrame(stack_std_model.transform(st_features.values), \n",
    "                                   index=st_features.index, columns=st_features.columns) \n",
    "    dct_std_tab_for_stack['set'+str(s)] = std_st_features #\n",
    "\n",
    "\n",
    "\n",
    "    bpar1, bpar2, acc, mse, corr, model, y_pred3, mae = elnet(std_st_features, y_res2) #ML\n",
    "\n",
    "    dict_st_perf1['set'+str(s)] = acc, mse, mae, corr, bpar1, bpar2 \n",
    "    dict_st_models['set'+str(s)] = model\n",
    "    dict_st_ypred1['set'+str(s)] = y_pred3\n",
    "    s+=1\n",
    "\n",
    "df_st_perf1 = pd.DataFrame(dict_st_perf1, index=['best score r2', 'mse', 'mae','corr', 'best alpha', 'best l1_ratio'])\n",
    "df_st_ypred1 = pd.DataFrame(dict_st_ypred1, index=y_res2.index)        \n",
    "\n",
    "###Save outputs from this step (models and all mod. perf.)\n",
    "\n",
    "#models\n",
    "for key in dict_st_models.keys():\n",
    "    joblib.dump(dict_st_models[key], (path_out+'/'+str(key)+'_stacked_model.sav'))\n",
    "for key in dct_std_mod_for_stack.keys():\n",
    "    joblib.dump(dct_std_mod_for_stack[key], (path_out+'/'+str(key)+'_stacked_STD_model.sav'))\n",
    "\n",
    "#performance and prediction\n",
    "df_st_perf1.to_csv(path_out+'/2level_test1_perf_stacked.csv')\n",
    "df_st_ypred1.to_csv(path_out+'/2level_test1_y_pred_stacked.csv')\n",
    "for key in dct_std_tab_for_stack.keys():\n",
    "    dct_std_tab_for_stack[key].to_csv(path_out+'/2level_stack_y_feature_tab_STD.csv')\n",
    "    dct_std_tab_before_for_stack[key].to_csv(path_out+'/2level_stack_y_feature_tab_beforeSTD.csv')\n",
    "\n",
    "\n",
    "\n",
    "### 3rd level ################################################################################\n",
    "print(' ')\n",
    "print('start 3rd level ', datetime.now())\n",
    "\n",
    "\n",
    "#### L3 Testing single ML models on test_index #############################################\n",
    "\n",
    "print('Checking single ML on test data ', datetime.now())\n",
    "\n",
    "#control for control table with sorting to test_index\n",
    "\n",
    "#control y (target)\n",
    "y_res3 = re_control_features(y, control, test_index, \n",
    "                             std_targ_y, std_targ_X, linreg_targ)\n",
    "\n",
    "#control modalities\n",
    "features_res3 = {}\n",
    "for key in features.keys(): \n",
    "    print('controlling ', key, datetime.now())\n",
    "\n",
    "    #features_res3[key] = re_control_features(features[key], control, y_res3.index, \n",
    "    #                                         std_feat_y_dct[key], std_feat_X_dct[key], linreg_feat_dct[key])\n",
    "    if key in ['emo', 'gam', 'lan', 'mot', 'rel', 'soc', 'wm']:\n",
    "        control_t = pd.concat([control, movements[key]], axis=1)\n",
    "        features_res3[key] = re_control_features(features[key], control_t, y_res3.index, \n",
    "                                             std_feat_y_dct[key], std_feat_X_dct[key], linreg_feat_dct[key])\n",
    "    else:\n",
    "        features_res3[key] = re_control_features(features[key], control, y_res3.index, \n",
    "                                             std_feat_y_dct[key], std_feat_X_dct[key], linreg_feat_dct[key])\n",
    "\n",
    "###standartize before model and keep std models\n",
    "#features\n",
    "for key in features_res3.keys():\n",
    "    print('standartize ', key, datetime.now())\n",
    "    features_res3[key] = pd.DataFrame(std_models_features[key].transform(features_res3[key].values),\n",
    "                                      index=features_res3[key].index, \n",
    "                                      columns=features_res3[key].columns)\n",
    "#target\n",
    "y_res3 = pd.DataFrame(std_model_target.transform(y_res3.values.reshape(-1, 1)),\n",
    "                      index=y_res3.index) \n",
    "\n",
    "#save features table before PCA\n",
    "y_res3.to_csv(path_out+'/target_y_test.csv')\n",
    "for key in features_res3.keys():\n",
    "    features_res3[key].to_csv(path_out+'/'+str(key)+'_test.csv')            \n",
    "\n",
    "\n",
    "#PCA models to rest and task FC\n",
    "for key in ['rest', 'gam_FC', 'lan_FC', 'mot_FC', 'rel_FC', 'soc_FC', 'wm_FC']:\n",
    "    print('reduction ', key, datetime.now())\n",
    "    features_res3[key] = pd.DataFrame(PCA_models[key].transform(features_res3[key].values), \n",
    "                              index=features_res3[key].index)\n",
    "\n",
    "\n",
    "#apply new std to PCA features again\n",
    "for key in ['rest', 'gam_FC', 'lan_FC', 'mot_FC', 'rel_FC', 'soc_FC', 'wm_FC']:\n",
    "    print('standartize PCA ', key, datetime.now())\n",
    "    features_res3[key] = pd.DataFrame(std_PC_feature_models[key].transform(features_res3[key].values),\n",
    "                                      index=features_res3[key].index, \n",
    "                                      columns=features_res3[key].columns)\n",
    "    #save std pc table\n",
    "    features_res3[key].to_csv(path_out+'/'+key+'_PCA75_test.csv')\n",
    "\n",
    "\n",
    "\n",
    "#apply trained single models ElasticNet to new data , test_index\n",
    "\n",
    "dict_y_pred3={}\n",
    "dict_y_pred3_per={}\n",
    "for key in list(features_res3.keys()):\n",
    "    y_pred, y_real, ind_y, bacc, mse, corr, mae = reaply_ElNet(features_res3[key], y_res3, dict_elnet_model[key]) #ML\n",
    "    dict_y_pred3[key] = y_pred\n",
    "    dict_y_pred3_per[key] = bacc, mse, mae, corr\n",
    "\n",
    "df_y_pred3 = pd.DataFrame(dict_y_pred3, index=ind_y)\n",
    "df_y_pred3_per = pd.DataFrame(dict_y_pred3_per, index=['best score r2', 'mse', 'mae','corr'])\n",
    "\n",
    "\n",
    "###Save outputs from this step (models and all mod. perf.)\n",
    "\n",
    "#model performance\n",
    "df_y_pred3_per.to_csv(path_out+'/3level_test2_perf_elnet.csv')\n",
    "\n",
    "#list of first level targets (observed and predicted)\n",
    "df_y_pred3.to_csv(path_out+'/3level_test2_y_pred_singleML.csv')        \n",
    "\n",
    "\n",
    "#### L3 Testing stacked ML models on test_index #############################################\n",
    "\n",
    "print('Calculating stacked ML on test data ', datetime.now()) \n",
    "\n",
    "#apply trained stacked models ElasticNet to new data , test_index\n",
    "\n",
    "#for presetet sets\n",
    "dict_st_perf2={}\n",
    "dict_st_ypred2={}\n",
    "\n",
    "dct_std3_tab_for_stack = {} #\n",
    "dct_std3_tab_before_for_stack = {} #\n",
    "\n",
    "s=1\n",
    "for set_n in [set1, set2, set3, set4, set5, set6, set7, set8]:\n",
    "\n",
    "    ftrs = df_y_pred3.loc[:, set_n]\n",
    "    dct_std3_tab_before_for_stack['set'+str(s)] = ftrs\n",
    "\n",
    "    std_ftrs = pd.DataFrame(dct_std_mod_for_stack['set'+str(s)].transform(ftrs.values), \n",
    "                            index=ftrs.index,columns=ftrs.columns)\n",
    "    dct_std3_tab_for_stack['set'+str(s)] = std_ftrs\n",
    "\n",
    "    y_pred, y_real, ind_y, bacc, mse, corr, mae = reaply_ElNet(std_ftrs, y_res3, dict_st_models[('set'+str(s))]) #ML\n",
    "    dict_st_ypred2[('set'+str(s))] = y_pred\n",
    "    dict_st_perf2[('set'+str(s))] = bacc, mse, mae, corr\n",
    "    s+=1\n",
    "\n",
    "df_st_ypred2 = pd.DataFrame(dict_st_ypred2, index=ind_y)\n",
    "df_st_perf2 = pd.DataFrame(dict_st_perf2, index=['best score r2', 'mse', 'mae','corr'])        \n",
    "\n",
    "###Save outputs from this step (models and all mod. perf.)\n",
    "\n",
    "#performance and prediction\n",
    "df_st_perf2.to_csv(path_out+'/3level_test2_perf_stacked.csv')\n",
    "df_st_ypred2.to_csv(path_out+'/3level_test2_y_pred_stacked.csv') \n",
    "for key in dct_std3_tab_for_stack.keys():\n",
    "    dct_std3_tab_for_stack[key].to_csv(path_out+'/3level_stack_y_feature_tab_STD.csv')\n",
    "    dct_std3_tab_before_for_stack[key].to_csv(path_out+'/3level_stack_y_feature_tab_beforeSTD.csv')\n",
    "\n",
    "\n",
    "print(' ')\n",
    "print('finished to calculate the Fold #', i)\n",
    "print(datetime.now())\n",
    "\n",
    "i+=1\n",
    "\n",
    "print(' ')\n",
    "print('finished the MODEL '+COL)\n",
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
